{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import happytransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 654/2988 [01:44<06:11,  6.28it/s]\n",
      "01/05/2024 07:25:29 - INFO - happytransformer.happy_transformer -   Using device: cpu\n",
      "01/05/2024 07:25:29 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/nicho/.cache/huggingface/datasets/csv/default-14bcef5a1b06b1b0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 124.96it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/nicho/.cache/huggingface/datasets/csv/default-14bcef5a1b06b1b0/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.32it/s]\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 2988/2988 [06:24<00:00,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loss: 1.2803850173950195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from happytransformer import HappyTextToText\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
    "eval_dataset = load_dataset(\"jfleg\", split='test[:]')\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "def remove_excess_spaces(text):\n",
    "    for rep in replacements:\n",
    "        text = text.replace(rep[0], rep[1])\n",
    "\n",
    "    return text\n",
    "\n",
    "def generate_csv(csv_path, dataset):\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writter = csv.writer(csvfile)\n",
    "        writter.writerow([\"input\", \"target\"])\n",
    "        for case in dataset:\n",
    "             # Adding the task's prefix to input \n",
    "            input_text = \"grammar: \" + case[\"sentence\"]\n",
    "            for correction in case[\"corrections\"]:\n",
    "                # a few of the cases contain blank strings. \n",
    "                if input_text and correction:\n",
    "                    writter.writerow([input_text, correction])\n",
    "\n",
    "\n",
    "\n",
    "generate_csv(\"train.csv\", train_dataset)\n",
    "generate_csv(\"eval.csv\", eval_dataset)\n",
    "before_result = happy_tt.eval(\"eval.csv\")\n",
    "print(\"Before loss:\", before_result.loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/nicho/.cache/huggingface/datasets/csv/default-28a6cf32e733df69/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 83.31it/s]\n",
      "01/05/2024 07:32:53 - INFO - happytransformer.happy_transformer -   Tokenizing training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/nicho/.cache/huggingface/datasets/csv/default-28a6cf32e733df69/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2024 07:32:59 - INFO - happytransformer.happy_transformer -   Moving model to cpu\n",
      "c:\\Users\\nicho\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "  0%|          | 1/340 [00:04<26:40,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4503, 'learning_rate': 4.985294117647059e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "  0%|          | 1/340 [00:45<26:40,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1107882261276245, 'eval_runtime': 40.6223, 'eval_samples_per_second': 7.434, 'eval_steps_per_second': 0.935, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 34/340 [03:06<22:55,  4.49s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8839, 'learning_rate': 4.5e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 10%|█         | 34/340 [03:47<22:55,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6632149815559387, 'eval_runtime': 40.9111, 'eval_samples_per_second': 7.382, 'eval_steps_per_second': 0.929, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 68/340 [06:19<17:57,  3.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.728, 'learning_rate': 4e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 68/340 [07:00<17:57,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5928208231925964, 'eval_runtime': 41.1872, 'eval_samples_per_second': 7.332, 'eval_steps_per_second': 0.923, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 102/340 [09:39<16:34,  4.18s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.65, 'learning_rate': 3.5e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 30%|███       | 102/340 [10:20<16:34,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5545536875724792, 'eval_runtime': 40.9118, 'eval_samples_per_second': 7.382, 'eval_steps_per_second': 0.929, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 136/340 [13:04<14:38,  4.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.667, 'learning_rate': 3e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 136/340 [13:49<14:38,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5355919599533081, 'eval_runtime': 44.6274, 'eval_samples_per_second': 6.767, 'eval_steps_per_second': 0.851, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 170/340 [16:24<12:42,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5947, 'learning_rate': 2.5e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|█████     | 170/340 [17:05<12:42,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5257046818733215, 'eval_runtime': 41.1291, 'eval_samples_per_second': 7.343, 'eval_steps_per_second': 0.924, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 204/340 [19:35<11:37,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5634, 'learning_rate': 2e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|██████    | 204/340 [20:18<11:37,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5134497284889221, 'eval_runtime': 42.4452, 'eval_samples_per_second': 7.115, 'eval_steps_per_second': 0.895, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 238/340 [22:55<07:13,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5918, 'learning_rate': 1.5e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|███████   | 238/340 [23:38<07:13,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5027403235435486, 'eval_runtime': 43.3028, 'eval_samples_per_second': 6.974, 'eval_steps_per_second': 0.878, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 272/340 [26:19<04:31,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6092, 'learning_rate': 1e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 272/340 [27:03<04:31,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4997316598892212, 'eval_runtime': 43.7047, 'eval_samples_per_second': 6.91, 'eval_steps_per_second': 0.869, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 306/340 [29:53<03:08,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5924, 'learning_rate': 5e-06, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 90%|█████████ | 306/340 [30:37<03:08,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5001949071884155, 'eval_runtime': 43.9546, 'eval_samples_per_second': 6.871, 'eval_steps_per_second': 0.865, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [33:05<00:00,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5619, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 340/340 [33:48<00:00,  5.97s/it]\n",
      "01/05/2024 08:06:48 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49949100613594055, 'eval_runtime': 43.4917, 'eval_samples_per_second': 6.944, 'eval_steps_per_second': 0.874, 'epoch': 1.0}\n",
      "{'train_runtime': 2028.7365, 'train_samples_per_second': 1.338, 'train_steps_per_second': 0.168, 'train_loss': 0.6458843757124508, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]\n",
      "100%|██████████| 2988/2988 [06:36<00:00,  7.53it/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After loss:  0.4827503263950348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import TTTrainArgs\n",
    "\n",
    "args = TTTrainArgs(batch_size=8)\n",
    "happy_tt.train(\"train.csv\", args=args)\n",
    "before_loss = happy_tt.eval(\"eval.csv\")\n",
    "print(\"After loss: \", before_loss.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/05/2024 00:04:12 - INFO - happytransformer.happy_transformer -   Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText\n",
    "\n",
    "# Create a Happy Text-to-Text model\n",
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\")\n",
    "#happy_tt.save(\"C:\\\\Users\\\\nicho\\\\OneDrive\\\\Documents\\\\KULIAHHHHHHHH\\\\Semester 3\\\\AI\\\\SpellSensemodel3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many peoples who do not realize the importance of learning new languages . Them think that its unnecessary and don't give it much importance . But, learning new languages opens up a whole new world of opportunities and helps us to communicate with peoples from different cultures and backgrounds . It also improves cognitive skills and gives us a broader perspective on the world .\n",
      "I am enjoying writing articles on AI.\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import TTSettings\n",
    "\n",
    "beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=100)\n",
    "\n",
    "example_1 = \"grammar: Their are many peoples who does not realizing the importance of learning new languages. Them think that its unnecessary and don't gives it much importance. But, learning new languages opens up a whole new world of opportunities and helps us to communicating with peoples from different cultures and backgrounds. Its also improves cognitive skills and gives us a broader perspective on the world.\"\n",
    "result_1 = happy_tt.generate_text(example_1, args=beam_settings)\n",
    "print(result_1.text)\n",
    "\n",
    "example_2 = \"grammar: I am enjoys, writtings articles ons AI.\"\n",
    "\n",
    "result_2 = happy_tt.generate_text(example_2, args=beam_settings)\n",
    "print(result_2.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18839a57e9d3ea0075fc8c6acfa22ad1d20052c052db5574a5f44cef4cff8e59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
